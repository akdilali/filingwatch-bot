import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import os
import json
import time
import re
import logging
from datetime import datetime
from main_v2 import post_tweet, get_x_client, KNOWN_TICKERS

# --- CONFIG ---
SEC_RSS_URL = "https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=D&output=atom"
USER_AGENT = "FilingWatch Bot (bot@filingwatch.com)" # SEC requires this!
STATE_FILE = "sec_state.json"
MIN_AMOUNT = 20_000_000 # $20 Million Whale Filter

# --- LOGGING ---
logging.basicConfig(
    filename='sec_bot.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger()
console = logging.StreamHandler()
console.setLevel(logging.INFO)
logger.addHandler(console)

class SECMonitor:
    def __init__(self):
        self.headers = {'User-Agent': USER_AGENT}
        self.last_link = self.load_state()

    def load_state(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get('last_link')
            except:
                pass
        return None

    def save_state(self, link):
        try:
            with open(STATE_FILE, 'w') as f:
                json.dump({'last_link': link, 'updated_at': datetime.now().isoformat()}, f)
        except Exception as e:
            logger.error(f"State save error: {e}")

    def get_filings(self):
        """RSS feedinden son formlarÄ± Ã§eker"""
        try:
            resp = requests.get(SEC_RSS_URL, headers=self.headers, timeout=20)
            if resp.status_code != 200:
                logger.error(f"SEC RSS Error: {resp.status_code}")
                return []
            
            # Atom Feed Parsing
            # XML namespace sorunu yaÅŸamamak iÃ§in basit string iÅŸlemi veya feedparser kullanÄ±labilir
            # Burada ElementTree ile namespace'i handle ederek gidelim
            root = ET.fromstring(resp.content)
            entries = []
            
            # Atom namespace usually: {http://www.w3.org/2005/Atom}
            # BasitÃ§e taglerde 'entry' arayalÄ±m
            for child in root:
                if 'entry' in child.tag:
                    entries.append(child)
                    
            filings = []
            for entry in entries:
                f = {}
                for node in entry:
                    if 'title' in node.tag: f['title'] = node.text
                    if 'link' in node.tag: f['link'] = node.attrib.get('href')
                    if 'summary' in node.tag: f['summary'] = node.text
                    if 'updated' in node.tag: f['date'] = node.text
                
                # Sadece Form D (zaten URL filter var ama teyit edelim)
                if 'D' in f.get('title', ''):
                    filings.append(f)
            
            return filings # En eskiden yeniye veya tam tersi. RSS genelde Yeni->Eski verir.
            
        except Exception as e:
            logger.error(f"RSS Parse Error: {e}")
            return []

    def get_details(self, filing_link):
        """Detay sayfasÄ±na gidip MiktarÄ± Ã§eker"""
        # filing_link ÅŸuna benzer: https://www.sec.gov/Archives/edgar/data/123/000123...-index.htm
        # Bizim asÄ±l XML/HTML dokÃ¼manÄ±na ihtiyacÄ±mÄ±z var.
        # Index sayfasÄ±nÄ± scrape edip "Primary Document" tablosundan ilk linki alacaÄŸÄ±z.
        
        try:
            # 1. Index SayfasÄ±
            resp = requests.get(filing_link, headers=self.headers, timeout=10)
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # Primary Document linkini bul
            # Tablo class="tableFile"
            doc_link = None
            for row in soup.find_all('tr'):
                cols = row.find_all('td')
                if len(cols) > 3:
                    # Column 3 = Document (index 2), Column 4 = Type (index 3)
                    doc_type = cols[3].get_text().strip()
                    if doc_type == 'D':
                        # Link index 2
                        a_tag = cols[2].find('a')
                        if a_tag:
                            href = a_tag['href']
                            # Ã–ncelik XML, yoksa HTML
                            current_link = "https://www.sec.gov" + href
                            if not doc_link or href.endswith('.xml'):
                                doc_link = current_link
                                if href.endswith('.xml'): 
                                    break # XML bulduk, tamamdÄ±r
            
            if not doc_link:
                return None, 0

            # 2. DokÃ¼manÄ± Ã‡ek
            time.sleep(0.2) # Rate limit nezaketi
            resp_doc = requests.get(doc_link, headers=self.headers, timeout=10)
            content = resp_doc.text
            
            # 3. Regex ile "Total Amount Sold" bul
            # XML pattern: <totalAmountSold>1000000</totalAmountSold>
            # HTML pattern: "Total Amount Sold" ... "$ 1,000,000"
            
            amount = 0
            
            # Pattern 1: XML
            m_xml = re.search(r'<totalAmountSold>[^<]*\$?([0-9,]+)[^<]*</totalAmountSold>', content, re.IGNORECASE)
            if m_xml:
                clean_str = m_xml.group(1).replace(',', '')
                try: amount = float(clean_str)
                except: pass
            
            # Pattern 2: Text Search (Fallback)
            if amount == 0:
                # "Total Amount Sold" yazÄ±sÄ±nÄ± bul
                idx = content.find("Total Amount Sold")
                if idx != -1:
                    snippet = content[idx:idx+200]
                    # "$ 350,000" veya "350000"
                    # Basit regex: $ ve rakamlar
                    nums = re.findall(r'\$?\s?([0-9,]{4,})', snippet)
                    if nums:
                        try: amount = float(nums[0].replace(',', ''))
                        except: pass

            return doc_link, amount

        except Exception as e:
            logger.error(f"Details Parse Error: {e}")
            return None, 0

    def format_amount(self, amount):
        if amount >= 1_000_000_000:
            return f"${amount/1_000_000_000:.1f} Billion"
        elif amount >= 1_000_000:
            return f"${amount/1_000_000:.1f} Million"
        else:
            return f"${amount:,.0f}"

    def run(self):
        logger.info("SEC Bot taramasÄ± baÅŸladÄ±...")
        filings = self.get_filings()
        
        # Yeni -> Eski geliyor. Terse Ã§evirelim ki eskiden yeniye tweet atalÄ±m (kronolojik)
        filings.reverse()
        
        new_last_link = self.last_link
        processed_count = 0
        
        # EÄŸer ilk Ã§alÄ±ÅŸtÄ±rÄ±ÅŸsa, hepsini tweet atma, sadece sonuncuyu iÅŸaretle
        if not self.last_link and filings:
            self.save_state(filings[-1]['link'])
            logger.info("Ä°lk kurulum: State kaydedildi, tweet atÄ±lmadÄ±.")
            return

        for f in filings:
            link = f['link']
            title = f['title'] # Ã–rn: "Form D - OPENAI INC (000...)"
            
            # State kontrolÃ¼: EÄŸer bu link daha Ã¶nce iÅŸlendiyse (veya ondan Ã¶ncekilerse) atla
            # Basit mantÄ±k: Link eÅŸitse dur (yeniye doÄŸru gidiyorduk... actually reverse ettik)
            # Logic: We process everything NEWER than last_link.
            # RSS linkleri unique id gibidir.
            
            # Ancak RSS listesinde last_link'i bulup sonrasÄ±nÄ± almak daha gÃ¼venli.
            # Biz basitÃ§e: EÄŸer link == last_link ise, bu noktaya kadar olanlarÄ± zaten iÅŸledik (reverse listede).
            # HayÄ±r, reverse listede: [Eski .... Last ... Yeni ... Yeni]
            # Flag mantÄ±ÄŸÄ± kuralÄ±m.
            pass 
        
        # Daha saÄŸlam mantÄ±k:
        # RSS'teki sÄ±rayÄ± (Yeni->Eski) kullanalÄ±m. Last_link'i gÃ¶rene kadar toplayalÄ±m.
        # Sonra toplananlarÄ± (Yenileri) ters Ã§evirip tweet atalÄ±m.
        
        filings = self.get_filings() # Tekrar al (Yeni -> Eski)
        new_entries = []
        
        for f in filings:
            if f['link'] == self.last_link:
                break
            new_entries.append(f)
            
        if not new_entries:
            logger.info("Yeni SEC bildirimi yok.")
            return

        logger.info(f"{len(new_entries)} yeni bildirim var. Detaylar Ã§ekiliyor...")
        
        # Eskiden yeniye iÅŸle
        for f in reversed(new_entries):
            # Åirket ismini al: "D - Company Name (CIK)" -> "Company Name"
            # Title format: "D - OpenAI, Inc. (0001956665) (Filer)"
            company_name = "Unknown Company"
            try:
                parts = f['title'].split('-')
                if len(parts) > 1:
                    raw_name = parts[1].strip()
                    # Parantezleri temizle (CIK kodu vs)
                    company_name = re.sub(r'\s*\(.*?\)', '', raw_name).strip()
            except:
                pass
                
            doc_link, amount = self.get_details(f['link'])
            
            if amount < MIN_AMOUNT:
                logger.info(f"Skipped {company_name}: ${amount:,.0f} < ${MIN_AMOUNT:,.0f}")
                new_last_link = f['link'] # Yine de ilerle
                continue
                
            # WHALE ALERT! ğŸ‹
            emoji = "ğŸ’°"
            if amount >= 100_000_000:
                emoji = "ğŸ‹"

            tweet = f"{emoji} NEW SEC FILING ALERT\n\nğŸ¢ {company_name}\nğŸ’µ {self.format_amount(amount)} Raised\n\nğŸ“„ Form D (Private Placement)\nğŸ”— {doc_link}"
            
            # Ticker check?
            # Ã–zel ÅŸirketlerde ticker olmaz ama yine de check edelim
            
            logger.info(f"YayÄ±nlanÄ±yor: {company_name} - {amount}")
            try:
                post_tweet(tweet)
                time.sleep(5) # Flood yapma
            except Exception as e:
                logger.error(f"Tweet error: {e}")
                
            new_last_link = f['link']
            
        # En son iÅŸlenen linki kaydet
        self.save_state(new_last_link)
        logger.info("SEC taramasÄ± tamamlandÄ±.")

if __name__ == "__main__":
    bot = SECMonitor()
    bot.run()
