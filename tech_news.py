import requests
from bs4 import BeautifulSoup
import logging
import sys
import os
# Import existing Twitter client
from main_v2 import post_tweet, get_x_client

# Config
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('TechNews')

class TechNewsBot:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def scrape_techmeme(self):
        """Techmeme ana manÅŸeti Ã§eker (RSS Ã¼zerinden)"""
        url = "https://www.techmeme.com/feed.xml"
        try:
            import xml.etree.ElementTree as ET
            resp = requests.get(url, headers=self.headers, timeout=10)
            root = ET.fromstring(resp.content)
            
            # Ä°lk Ã¶ÄŸe her zaman manÅŸettir
            item = root.find('./channel/item')
            if not item:
                logger.error("Techmeme RSS boÅŸ.")
                return None
            
            title = item.find('title').text
            link = item.find('link').text
            
            # Description HTML iÃ§eriyor, metni ayÄ±klayalÄ±m
            description_html = item.find('description').text
            soup = BeautifulSoup(description_html, 'html.parser')
            clean_desc = soup.get_text().strip()
            
            # Tweet Formatla
            tweet = f"ðŸ“° TECHMEME MANÅžET\n\nðŸš¨ {title}\n\n{clean_desc[:100]}...\n\nðŸ”— {link}\n\n#TechNews #Breaking #Technology"
            logger.info(f"Techmeme bulundu: {title}")
            return tweet
            
        except Exception as e:
            logger.error(f"Techmeme Scrape HatasÄ±: {e}")
            return None

    def scrape_producthunt(self):
        """Product Hunt gÃ¼nÃ¼n Ã¼rÃ¼nÃ¼nÃ¼ Ã§eker (RSS Ã¼zerinden)"""
        url = "https://www.producthunt.com/feed"
        try:
            import xml.etree.ElementTree as ET
            resp = requests.get(url, headers=self.headers, timeout=10)
            root = ET.fromstring(resp.content)
            
            # Ä°lk Ã¶ÄŸe (En gÃ¼ncel/popÃ¼ler)
            # RSS namespace kullanabilir, basit find ile deneyelim
            item = root.find('./channel/item')
            if not item:
                logger.error("Product Hunt RSS boÅŸ.")
                return None
                
            title = item.find('title').text
            link = item.find('link').text
            description = item.find('description').text
            
            # HTML tagleri temizle (basitÃ§e)
            soup = BeautifulSoup(description, 'html.parser')
            clean_desc = soup.get_text().strip()
            
            tweet = f"ðŸš€ PRODUCT HUNT GÃœNÃœN ÃœRÃœNÃœ\n\nâœ¨ {title}\n\nðŸ’¡ {clean_desc[:120]}...\n\nðŸ”— {link}\n\n#ProductHunt #NewTool #Startup"
            logger.info(f"PH bulundu: {title}")
            return tweet
            
        except Exception as e:
            logger.error(f"Product Hunt Scrape HatasÄ±: {e}")
            return None

    def scrape_github_trending(self):
        """GitHub Trending #1 reposunu Ã§eker"""
        url = "https://github.com/trending"
        try:
            resp = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # Ä°lk Article (Box-row)
            article = soup.find('article', class_='Box-row')
            if not article:
                logger.error("GitHub Trending yapÄ±sÄ± deÄŸiÅŸmiÅŸ.")
                return None
                
            # Repo Name (h2 > a) -> "user / repo"
            h2 = article.find('h2')
            if not h2: return None
            
            repo_link_tag = h2.find('a')
            repo_name = repo_link_tag.get_text().strip().replace('\n', '').replace(' ', '')
            repo_path = repo_link_tag['href']
            link = f"https://github.com{repo_path}"
            
            # Description (p)
            p = article.find('p')
            desc = p.get_text().strip() if p else "No description."
            
            # Stars Today (En sondaki span)
            # YapÄ±: <span class="d-inline-block float-sm-right"> <svg>...</svg> 123 stars today </span>
            # BasitÃ§e metin iÃ§inde "stars today" arayalÄ±m
            all_text = article.get_text()
            stars_today = "Unknown"
            if "stars today" in all_text:
                # SatÄ±rlara bÃ¶l ve bul
                for line in all_text.split('\n'):
                     if "stars today" in line:
                         stars_today = line.strip().replace(' stars today', '')
                         break
            
            # Tweet
            tweet = f"ðŸ”¥ GITHUB TRENDING #1\n\nðŸ“¦ {repo_name}\nâ­ {stars_today} stars today\n\nðŸ’¡ {desc[:100]}...\n\nðŸ”— {link}\n\n#OpenSource"
            
            # Dinamik Etiketler (Ä°Ã§eriÄŸe gÃ¶re)
            desc_lower = desc.lower()
            if any(x in desc_lower for x in ['llm', 'gpt', 'ai ', 'artificial intelligence', 'model', 'neural']):
                tweet += " #AI"
            if 'game' in desc_lower:
                tweet += " #GameDev"
            if 'web' in desc_lower or 'css' in desc_lower or 'react' in desc_lower:
                 tweet += " #WebDev"
                
            logger.info(f"GitHub Trending bulundu: {repo_name}")
            return tweet
            
        except Exception as e:
            logger.error(f"GitHub Scrape HatasÄ±: {e}")
            return None

    def run(self, source: str):
        logger.info(f"Bot Ã§alÄ±ÅŸÄ±yor... Kaynak: {source}")
        tweet_text = None
        
        if source == 'techmeme':
            tweet_text = self.scrape_techmeme()
        elif source == 'producthunt':
            tweet_text = self.scrape_producthunt()
        elif source == 'github':
            tweet_text = self.scrape_github_trending()
        else:
            logger.error("GeÃ§ersiz kaynak. SeÃ§enekler: techmeme, producthunt, github")
            return

        if tweet_text:
            # Tweet at
            try:
                # post_tweet fonksiyonunu main_v2'den Ã§aÄŸÄ±rÄ±yoruz
                # Not: post_tweet iÃ§inde 'print' var, loglama iÃ§in yeterli.
                # Safe check for length
                if len(tweet_text) > 280:
                    tweet_text = tweet_text[:277] + "..."
                    
                logger.info(f"Tweetleniyor:\n{tweet_text}")
                post_tweet(tweet_text) # GerÃ§ek tweet
            except Exception as e:
                logger.error(f"Tweet atma hatasÄ±: {e}")
        else:
            logger.warning("Ä°Ã§erik bulunamadÄ±, tweet atÄ±lmadÄ±.")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("KullanÄ±m: python tech_news.py [techmeme|producthunt]")
        sys.exit(1)
        
    source_arg = sys.argv[1]
    bot = TechNewsBot()
    bot.run(source_arg)
